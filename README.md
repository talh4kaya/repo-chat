# ğŸ§  Repo-Chat: Local RAG with Llama 3

![Python](https://img.shields.io/badge/Python-3.9-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.3-green.svg)
![Ollama](https://img.shields.io/badge/Model-Llama3-orange.svg)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-red.svg)

**Repo-Chat**, GitHub Ã¼zerindeki herhangi bir kod tabanÄ±nÄ± indirip analiz eden ve **yerel yapay zeka (Local LLM)** kullanarak kodlarÄ±nÄ±zla sohbet etmenizi saÄŸlayan bir RAG (Retrieval-Augmented Generation) asistanÄ±dÄ±r.

Bu proje, verilerinizi 3. parti sunuculara (OpenAI vb.) gÃ¶ndermeden, tamamen kendi bilgisayarÄ±nÄ±zda (Offline & Private) Ã§alÄ±ÅŸÄ±r.

---

## ğŸš€ Ã–zellikler

* **ğŸ”’ %100 Gizlilik:** KodlarÄ±nÄ±z bilgisayarÄ±nÄ±zdan dÄ±ÅŸarÄ± Ã§Ä±kmaz.
* **ğŸ§  Llama 3 GÃ¼cÃ¼:** Meta'nÄ±n en son teknoloji aÃ§Ä±k kaynak modelini kullanÄ±r.
* **âš¡ VektÃ¶r Arama:** ChromaDB ile kodlar arasÄ±nda anlamsal arama yapar.
* **ğŸ’¬ Modern ArayÃ¼z:** Streamlit ile geliÅŸtirilmiÅŸ, temiz ve kullanÄ±cÄ± dostu chat ekranÄ±.

---

## ğŸ›  Kurulum

### 1. Gereksinimler
* **Python 3.9+**
* **Ollama** (Llama 3 modelini Ã§alÄ±ÅŸtÄ±rmak iÃ§in)

### 2. Kurulum AdÄ±mlarÄ±

```bash
# 1. Repoyu klonlayÄ±n
git clone [https://github.com/KULLANICI_ADIN/repo-chat.git](https://github.com/KULLANICI_ADIN/repo-chat.git)
cd repo-chat

# 2. Sanal ortamÄ± kurun
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. KÃ¼tÃ¼phaneleri yÃ¼kleyin
pip install -r requirements.txt



Harika bir fikir! ğŸ¨ Ã‡alÄ±ÅŸan bir sistemi "SatÄ±labilir bir ÃœrÃ¼ne" dÃ¶nÃ¼ÅŸtÃ¼rmenin yolu, KullanÄ±cÄ± Deneyimi (UI/UX) ve Sunumdan (README) geÃ§er.

Åu anki arayÃ¼z biraz "ham". Onu modern, ferah (beyaz tema) ve profesyonel bir Chatbot gÃ¶rÃ¼nÃ¼mÃ¼ne kavuÅŸturacaÄŸÄ±z. AyrÄ±ca GitHub vitrinini de sÃ¼sleyeceÄŸiz.

Hadi projeye son makyajÄ± yapalÄ±m.

ğŸ¨ ADIM 1: Modern ve Beyaz Tema AyarÄ±
Streamlit varsayÄ±lan olarak sistem temasÄ±nÄ± (karanlÄ±k/aydÄ±nlÄ±k) kullanÄ±r. Biz bunu zorla Beyaz/AydÄ±nlÄ±k yapacaÄŸÄ±z ve renkleri gÃ¼zelleÅŸtireceÄŸiz.

Proje klasÃ¶rÃ¼nÃ¼n iÃ§inde .streamlit adÄ±nda (baÅŸÄ±nda nokta var) yeni bir klasÃ¶r oluÅŸtur.

O klasÃ¶rÃ¼n iÃ§ine config.toml adÄ±nda bir dosya oluÅŸtur.

Ä°Ã§ine ÅŸu ayarlarÄ± yapÄ±ÅŸtÄ±r:

Ini, TOML

[theme]
base = "light"
primaryColor = "#2563EB" # Profesyonel Mavi
backgroundColor = "#FFFFFF"
secondaryBackgroundColor = "#F0F2F6"
textColor = "#1F2937"
font = "sans serif"

[server]
headless = true
(Bu ayar, sitenin her zaman bembeyaz, temiz ve kurumsal gÃ¶rÃ¼nmesini saÄŸlar).

ğŸ’… ADIM 2: app.py MakyajÄ± (Profesyonel UI)
Mevcut app.py dosyanÄ± aÃ§ ve tamamen sil. Yerine aÅŸaÄŸÄ±daki geliÅŸtirilmiÅŸ kodu yapÄ±ÅŸtÄ±r.

Yenilikler:

Sidebar (Yan Panel): Ayarlar ve "Sohbeti Temizle" butonu eklendi.

Ã–zel CSS: Mesaj balonlarÄ± (WhatsApp tarzÄ±) eklendi.

Avatar: KullanÄ±cÄ± ve AI iÃ§in ikonlar.

Temiz DÃ¼zen: Gereksiz yazÄ±lar kaldÄ±rÄ±ldÄ±.

Python

import streamlit as st
import time
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# SAYFA AYARLARI
st.set_page_config(page_title="Repo-Chat AI", page_icon="ğŸ§ ", layout="centered")

# Ã–ZEL CSS (Mesaj BalonlarÄ± iÃ§in)
st.markdown("""
<style>
    .stChatMessage {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stChatMessage[data-testid="stChatMessageUser"] {
        background-color: #E3F2FD;
        border-left: 5px solid #2196F3;
    }
    .stChatMessage[data-testid="stChatMessageAssistant"] {
        background-color: #F5F5F5;
        border-left: 5px solid #4CAF50;
    }
    h1 {
        color: #1F2937;
        font-family: 'Helvetica', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# YAN PANEL (SIDEBAR)
# ---------------------------------------------------------
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/chatbot.png", width=80)
    st.title("Repo-Chat v1.0")
    st.caption("ğŸš€ Local RAG System")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ Model Bilgisi")
    st.info("ğŸ§  **Brain:** Llama 3 (8B)\nğŸ—‚ï¸ **Memory:** ChromaDB")
    
    if st.button("ğŸ—‘ï¸ Sohbeti Temizle", type="primary"):
        st.session_state.messages = []
        st.rerun()
        
    st.markdown("---")
    st.markdown("Developed by **Talha Kaya**")

# ---------------------------------------------------------
# RAG SÄ°STEMÄ° YÃœKLEME
# ---------------------------------------------------------
DB_PATH = "./chroma_db"

@st.cache_resource
def load_rag_system():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    llm = ChatOllama(model="llama3", temperature=0.1)
    
    template = """
    Sen uzman bir KÄ±demli YazÄ±lÄ±m MÃ¼hendisisin.
    AÅŸaÄŸÄ±daki kod parÃ§alarÄ±nÄ± kullanarak kullanÄ±cÄ±nÄ±n sorusunu teknik olarak cevapla.
    CevabÄ±n TÃ¼rkÃ§e olsun.
    
    Kodlar:
    {context}
    
    Soru: {question}
    
    Cevap:
    """
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
        return_source_documents=True
    )
    return qa_chain

try:
    qa = load_rag_system()
except Exception as e:
    st.error(f"Sistem yÃ¼klenemedi: {e}")
    st.stop()

# ---------------------------------------------------------
# ANA EKRAN
# ---------------------------------------------------------
st.title("ğŸ’¬ KodlarÄ±nla Sohbet Et")
st.caption("GitHub reponuzdaki kodlarÄ± analiz eder ve sorularÄ±nÄ±zÄ± yanÄ±tlar.")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Merhaba! KodlarÄ±nÄ±zÄ± inceledim. Bana mimari, fonksiyonlar veya hatalar hakkÄ±nda soru sorabilirsin. ğŸ‘‹"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar="ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"):
        st.markdown(message["content"])

if prompt := st.chat_input("Bir soru sor..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        message_placeholder = st.empty()
        with st.spinner("Kodlar taranÄ±yor..."):
            result = qa.invoke({"query": prompt})
            response = result["result"]
            
            sources = list(set([doc.metadata.get("source", "Bilinmiyor") for doc in result["source_documents"]]))
            
            full_response = f"{response}\n\n---\n**ğŸ“š Kaynak Dosyalar:**\n"
            for src in sources:
                # Dosya yolunu temizle (sadece dosya adÄ± kalsÄ±n)
                clean_src = src.split("\\")[-1].split("/")[-1]
                full_response += f"- `{clean_src}`\n"
            
            message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
ğŸ“ ADIM 3: requirements.txt OluÅŸturma
ArtÄ±k proje bittiÄŸine gÃ¶re, baÅŸkalarÄ±nÄ±n da Ã§alÄ±ÅŸtÄ±rabilmesi iÃ§in kÃ¼tÃ¼phane listesini Ã§Ä±karalÄ±m.

Terminalde:

Bash

pip freeze > requirements.txt
ğŸ“„ ADIM 4: README.md (Vitrin DÃ¼zenleme)
Projenin ana dizininde README.md adÄ±nda bir dosya oluÅŸtur ve aÅŸaÄŸÄ±daki Profesyonel TanÄ±tÄ±mÄ± yapÄ±ÅŸtÄ±r.

Markdown

# ğŸ§  Repo-Chat: Local RAG with Llama 3

![Python](https://img.shields.io/badge/Python-3.9-blue.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.3-green.svg)
![Ollama](https://img.shields.io/badge/Model-Llama3-orange.svg)
![Streamlit](https://img.shields.io/badge/UI-Streamlit-red.svg)

**Repo-Chat**, GitHub Ã¼zerindeki herhangi bir kod tabanÄ±nÄ± indirip analiz eden ve **yerel yapay zeka (Local LLM)** kullanarak kodlarÄ±nÄ±zla sohbet etmenizi saÄŸlayan bir RAG (Retrieval-Augmented Generation) asistanÄ±dÄ±r.

Bu proje, verilerinizi 3. parti sunuculara (OpenAI vb.) gÃ¶ndermeden, tamamen kendi bilgisayarÄ±nÄ±zda (Offline & Private) Ã§alÄ±ÅŸÄ±r.

---

## ğŸš€ Ã–zellikler

* **ğŸ”’ %100 Gizlilik:** KodlarÄ±nÄ±z bilgisayarÄ±nÄ±zdan dÄ±ÅŸarÄ± Ã§Ä±kmaz.
* **ğŸ§  Llama 3 GÃ¼cÃ¼:** Meta'nÄ±n en son teknoloji aÃ§Ä±k kaynak modelini kullanÄ±r.
* **âš¡ VektÃ¶r Arama:** ChromaDB ile kodlar arasÄ±nda anlamsal arama yapar.
* **ğŸ’¬ Modern ArayÃ¼z:** Streamlit ile geliÅŸtirilmiÅŸ, temiz ve kullanÄ±cÄ± dostu chat ekranÄ±.

---

## ğŸ›  Kurulum

### 1. Gereksinimler
* **Python 3.9+**
* **Ollama** (Llama 3 modelini Ã§alÄ±ÅŸtÄ±rmak iÃ§in)

### 2. Kurulum AdÄ±mlarÄ±

```bash
# 1. Repoyu klonlayÄ±n
git clone [https://github.com/KULLANICI_ADIN/repo-chat.git](https://github.com/KULLANICI_ADIN/repo-chat.git)
cd repo-chat

# 2. Sanal ortamÄ± kurun
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. KÃ¼tÃ¼phaneleri yÃ¼kleyin
pip install -r requirements.txt
3. Modelin HazÄ±rlanmasÄ± (Ollama)
BilgisayarÄ±nÄ±zda Ollama'nÄ±n kurulu olduÄŸundan emin olun ve terminalden modeli Ã§ekin:

Bash

ollama run llama3
ğŸƒâ€â™‚ï¸ KullanÄ±m
AdÄ±m 1: KodlarÄ± HafÄ±zaya At (Ingestion)
Analiz etmek istediÄŸiniz GitHub reposunu ingest.py iÃ§indeki REPO_URL kÄ±smÄ±na yazÄ±n ve Ã§alÄ±ÅŸtÄ±rÄ±n:

Bash

python ingest.py
(Bu iÅŸlem kodlarÄ± indirir, parÃ§alar ve ChromaDB veritabanÄ±na kaydeder).

AdÄ±m 2: AsistanÄ± BaÅŸlat
Bash

streamlit run app.py
TarayÄ±cÄ±nÄ±zda aÃ§Ä±lan ekrandan kodlarÄ±nÄ±zla konuÅŸmaya baÅŸlayabilirsiniz! ğŸ‰

ğŸ— Mimari
Ingestion: gitpython ile repo indirilir.

Splitting: RecursiveCharacterTextSplitter ile kodlar parÃ§alanÄ±r.

Embedding: HuggingFaceEmbeddings ile vektÃ¶re Ã§evrilir.

Vector Store: ChromaDB Ã¼zerinde saklanÄ±r.

Retrieval & Chat: KullanÄ±cÄ± sorusu LangChain aracÄ±lÄ±ÄŸÄ±yla Llama 3'e iletilir ve en alakalÄ± kod parÃ§alarÄ±yla birlikte cevaplanÄ±r.