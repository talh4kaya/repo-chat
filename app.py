import streamlit as st
import time
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# SAYFA AYARLARI
st.set_page_config(page_title="Repo-Chat AI", page_icon="ğŸ§ ", layout="centered")

# Ã–ZEL CSS (Mesaj BalonlarÄ± iÃ§in)
st.markdown("""
<style>
    .stChatMessage {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stChatMessage[data-testid="stChatMessageUser"] {
        background-color: #E3F2FD;
        border-left: 5px solid #2196F3;
    }
    .stChatMessage[data-testid="stChatMessageAssistant"] {
        background-color: #F5F5F5;
        border-left: 5px solid #4CAF50;
    }
    h1 {
        color: #1F2937;
        font-family: 'Helvetica', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# YAN PANEL (SIDEBAR)
# ---------------------------------------------------------
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/chatbot.png", width=80)
    st.title("Repo-Chat v1.0")
    st.caption("ğŸš€ Local RAG System")
    
    st.markdown("---")
    st.markdown("### âš™ï¸ Model Bilgisi")
    st.info("ğŸ§  **Brain:** Llama 3 (8B)\nğŸ—‚ï¸ **Memory:** ChromaDB")
    
    if st.button("ğŸ—‘ï¸ Sohbeti Temizle", type="primary"):
        st.session_state.messages = []
        st.rerun()
        
    st.markdown("---")
    st.markdown("Developed by **Talha Kaya**")

# ---------------------------------------------------------
# RAG SÄ°STEMÄ° YÃœKLEME
# ---------------------------------------------------------
DB_PATH = "./chroma_db"

@st.cache_resource
def load_rag_system():
    embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    vectordb = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)
    llm = ChatOllama(model="llama3", temperature=0.1)
    
    template = """
    Sen uzman bir KÄ±demli YazÄ±lÄ±m MÃ¼hendisisin.
    AÅŸaÄŸÄ±daki kod parÃ§alarÄ±nÄ± kullanarak kullanÄ±cÄ±nÄ±n sorusunu teknik olarak cevapla.
    CevabÄ±n TÃ¼rkÃ§e olsun.
    
    Kodlar:
    {context}
    
    Soru: {question}
    
    Cevap:
    """
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)
    
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
        return_source_documents=True
    )
    return qa_chain

try:
    qa = load_rag_system()
except Exception as e:
    st.error(f"Sistem yÃ¼klenemedi: {e}")
    st.stop()

# ---------------------------------------------------------
# ANA EKRAN
# ---------------------------------------------------------
st.title("ğŸ’¬ KodlarÄ±nla Sohbet Et")
st.caption("GitHub reponuzdaki kodlarÄ± analiz eder ve sorularÄ±nÄ±zÄ± yanÄ±tlar.")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Merhaba! KodlarÄ±nÄ±zÄ± inceledim. Bana mimari, fonksiyonlar veya hatalar hakkÄ±nda soru sorabilirsin. ğŸ‘‹"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar="ğŸ§‘â€ğŸ’»" if message["role"] == "user" else "ğŸ¤–"):
        st.markdown(message["content"])

if prompt := st.chat_input("Bir soru sor..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ’»"):
        st.markdown(prompt)

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        message_placeholder = st.empty()
        with st.spinner("Kodlar taranÄ±yor..."):
            result = qa.invoke({"query": prompt})
            response = result["result"]
            
            sources = list(set([doc.metadata.get("source", "Bilinmiyor") for doc in result["source_documents"]]))
            
            full_response = f"{response}\n\n---\n**ğŸ“š Kaynak Dosyalar:**\n"
            for src in sources:
                # Dosya yolunu temizle (sadece dosya adÄ± kalsÄ±n)
                clean_src = src.split("\\")[-1].split("/")[-1]
                full_response += f"- `{clean_src}`\n"
            
            message_placeholder.markdown(full_response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})