from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage
import time

# 1. MODELÄ° BAÄLA
print("ğŸ”Œ Ollama'ya baÄŸlanÄ±lÄ±yor...")
# 'llama3' modelini kullanacaÄŸÄ±z. SÄ±caklÄ±ÄŸÄ± (temperature) 0.7 yaptÄ±k ki biraz yaratÄ±cÄ± olsun.
llm = ChatOllama(model="llama3", temperature=0.7)

# 2. SORUYU HAZIRLA
soru = "YazÄ±lÄ±m mÃ¼hendisliÄŸinde 'Bug' (BÃ¶cek) teriminin tarihÃ§esini 2 cÃ¼mleyle anlat."
messages = [HumanMessage(content=soru)]

print(f"ğŸ¤– Soru Soruluyor: {soru}")
print("â³ DÃ¼ÅŸÃ¼nÃ¼yor (Local GPU)...")

# 3. CEVABI AL
start_time = time.time()
response = llm.invoke(messages)
end_time = time.time()

# 4. SONUCU YAZDIR
print("-" * 50)
print(f"ğŸ’¬ CEVAP:\n{response.content}")
print("-" * 50)
print(f"âš¡ SÃ¼re: {end_time - start_time:.2f} saniye")